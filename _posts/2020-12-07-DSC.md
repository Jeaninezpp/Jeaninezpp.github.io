---
layout: post
title: Deep Subspace Clustering Networks论文
date: 2020-12-07 20:30:00 +0800
category: paper
tags: ["paper","deep clustering"]
thumbnail: /style/image/paper.jpg
icon: paper
---

文章提出一种结合自编码器和子空间聚类进行深度聚类的方法，可以很好的适用于解决非线性问题。文章通过自动编码器的编码层将数据非线性的映射到潜在空间，并创新性地在编码器和解码器之间引入一个自表达层，直接学习到相似度矩阵。

论文作者：Pan Ji(University of Adelaide) Tong Zhang、Hongdong Li (Australian National University) 等.
论文来源：NIPS

# 问题引入：
目前子空间聚类的相关工作都是基于线性子空间，然而现实世界中的数据并不一定符合线性子空间模型。
通常使用核技巧 (kernel trick) 来解决非线性子空间问题。
但是选择什么样的核，通常是根据经验。并且没有明确的解释可以说明预定义核对应的隐式特征空间是真正适合子空间聚类的。

# 文章贡献：
- 引入了一种新型的深度神经网络架构来学习显式的非线性映射，该映射可以很好地适应子空间聚类。
- 在编码器和解码器之间引入一个新的自表达层，编码了来自子空间的数据的“自表达”性质，即每个数据样本可以表示为同一子空间中其他样本的线性组合。 


# Method (DSC-Nets)
## 目标函数
- **自表达**
    - 自表达性质是指数据点可以通过与其在同一个子空间的其他数据点的线性组合来表示。表示为：$X=XC$。
其中C是自表达组合系数矩阵。假设子空间之间互相独立，那么通过最小化C的范数（公式1），可以保证C具有对角结构。这样就可以利用C来构造相似度矩阵。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200531174729618.png)

对C用的范数不同，可以延伸出不同的方法：
  - L1：Sparse Subspace Clustering (SSC)
  - 核范数：Low Rank Representation (LRR)、Low Rank Subspace Clustering(LRSC)
  - L2范数

- 有噪声和数据损坏的情况：$X=XC+E$，需要最小化$\|E\|_F$，也就是如下形式：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200531174754335.png)

> 但是，自表达性质只适用于线性空间。前面提到了核方法的一些缺点，因此文章结合深度自编码器以及自表达的思想，提出下面的方法。

- **基于自表达的深度自编码器**
希望训练一个深度自编码器可以学到适合子空间聚类的潜在表示。因此，引入一层编码自表达的概念。
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020053117481379.png)
损失函数中，有重建误差和自表达的误差和正则化项。
    - 在自表达这一项，表示每个潜在空间中的点$z_i$可以通过其他点$z_j$以权重$c_ij$进行线性组合来近似表示。这可以通过一个全连接层来表示。不进行非线性激活。
    - 其中，C采用了两种范数来进行试验。$L_1$和$L_2$。
    - 因此，为了解这个式子，我们可以把C看做一个网络层的参数$\Theta_s$。这样就可以通过反向传播联合优化自编码器的参数$\Theta$和$\Theta_s$了。如果采用交替优化$\Theta$和$\Theta_s$的方法，由于目标函数非凸，所以不能保证收敛。

**目标函数**转化为：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200531174839792.png)

## 网络结构
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200531174923788.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pwYWludGVy,size_16,color_FFFFFF,t_70)
- 包括三部分：编码器、自表达层、解码器。
- 采用卷积自动编码器。参数少，易训练。全连接的也可以用。
- 卷积层：2 $\times$ 2步长的核、ReLU非线性激活。
- 一次把所有的样本放进一个batch中。
- 参数量
	- 自动编码器的参数量：$\sum_i2k^2_i n_{i-1}n_i+\sum_i2n_i-n_1+1$
	- 样本数为N，自表达层参数为$N^2$

## 训练策略
直接训练具有数百万个参数的网络很困难。提出 预训练+微调 策略。
- 首先预训练没有自表达层的网络。然后用预训练的参数初始化编码器和解码器。
- 微调阶段：大batch所有数据，用梯度下降来最小化损失函数。用Adam。
- 网络训练好后用自表示层的参数构造**相似度矩阵**。$|C|+|C^T|$
- 最后进行某种聚类算法得到聚类结果。

# Experiments
- 数据集：
face clustering Dataset：Extended Yale B [21] and ORL [39]
general object clustering Dataset：COIL20 [31] and COIL100 [30].

- Extended Yale B
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200601154457489.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200601154517290.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200601154546227.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pwYWludGVy,size_16,color_FFFFFF,t_70)

- ORL
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200601154608927.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200601154939822.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200601155245277.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pwYWludGVy,size_16,color_FFFFFF,t_70)

- COIL20 and COIL100 Datasets
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200601155602621.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200601160054581.png)

# 总结
使用深度自编码器和自表达层直接学习到相似度矩阵，来做子空间聚类，并将自表达系数组作为可以训练的参数。但是这导致网络参数是样本个数的平方，可扩展性有待增强。
