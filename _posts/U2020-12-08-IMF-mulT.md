# MyConclusion:
- 似乎 (Tsai et al., 2019) and (Liu et al., 2018) 更有价值去看一下。

# Introduction
- sentiment和emotion？
    - The videos are cut into continuous segments and the segments are anno-tated with 7 point scale sentiment labels and 4 point scale emotion categories corresponding to the Ekman’s 6 basic emotion classes (Ekman, 2002). 
    - 4 emotions (happy, sad, angry, neutral).
- tensor-based multimodal fusion on emotion understanding.
- 结合T和L的想法，探索使用tranformer模型来处理对齐和未对齐的信号，没有对模型进行过参数化，使用了多个模态特定的transformer。利用基于low rank matrix factorization的融合方法来表示多模态的融合。
## 创新点：
- mulT中有unimodal和bimodal，没有trimodel。文章比mulT使用更少的transformer和更少的并行模型实现了相同的多模态表示。
- 基于unaligned sequence的模型进行学习的方法来模仿人类多模态语言表达的协调性，比强制信号同步的方法要好。
- LMF-MulT(文章方法)：the fused multimodal signal is reinforced using attention from the 3 modalities. 融合的信号通过3个模态的注意力加强。
    - Fusion-Based-CM-Attn：the individual modalities are reinforced in parallel via the fused signal.各个模态通过融合信号并行加强。
- The LMF method aims to capture all unimodal, bimodal and trimodal interactions amongst the modalities via approximate Tensor Fusion method（近似张量融合方法）. 
# Method
## Low Rank Fusion
- LMF（low-rank matrix factorization）：张量融合方法，可以对单模态、双模态和三模态相互作用进行建模，而不需要从模态特定的嵌入表示 使用昂贵的3倍笛卡尔乘积（Zadeh等，2017）。
- 采用 (Liu et al., 2018) 中方法。
![20201208152633](https://jeanine-1304440691.cos.ap-chengdu.myqcloud.com/20201208152633.png)
    - LMF原理？
    - h如何得到？
    使用lowo-rank modality-specific factors和模态的表示Z相乘后再乘。
## Multimodal Transformer
在基于 Transformers 的序列编码的基础上，利用 Tsai 的 multiple cross-modal attention blocks 模块，然后用 self-attention 来编码多模态序列做分类。

- 早期工作关注一个模态到另外一个模态的潜在适应。文本关注利用single-head(?) 和 cross-modal attention 实现潜在多模态自身对单个模态的适应。
- LMF 后加时间卷积。
    ![20201208203957](https://jeanine-1304440691.cos.ap-chengdu.myqcloud.com/20201208203957.png)
    
    ![20201208204239](https://jeanine-1304440691.cos.ap-chengdu.myqcloud.com/20201208204239.png)


# Experiment

可以达到和MulT comparable 的结果，但是相比它，文章具有以下优点：
- 没有为不同的模态使用多个并行的 self-attention transformer.
- 使用的 transformer 数量少，时间短。
- 训练参数少，可达到相似的性能。
